{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Shreeya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/Shreeya/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.probability import FreqDist\n",
    "import re\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "from nltk import pos_tag\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering Input\n",
    "1. **Tokenization**: The word_tokenize function is applied to the pitch text, splitting it into individual characters.\n",
    "\n",
    "2. **Filtering and Processing**: The code removes punctuation and stopwords from the tokenized pitch using regular expressions (punctuation.sub) and a list of stopwords from the NLTK library (stopwords.words('english')).\n",
    "\n",
    "3. **Lemmatization**: The code utilizes the WordNetLemmatizer from NLTK (WordNetLemmatizer()) to lemmatize the list of words with no punctuation. Lemmatization reduces words to their root form. The resulting lemmas are stored in the filtered_pitch list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi',\n",
       " 'name',\n",
       " 'shreeya',\n",
       " 'kantamsetty',\n",
       " 'i',\n",
       " 'rising',\n",
       " 'sophomore',\n",
       " 'i',\n",
       " 'interested',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'data',\n",
       " 'science',\n",
       " 'i',\n",
       " 'expertise',\n",
       " 'web',\n",
       " 'development',\n",
       " 'angular',\n",
       " 'cs',\n",
       " 'html',\n",
       " 'html',\n",
       " 'html',\n",
       " 'html',\n",
       " 'html']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pitch = \"Hi, my name is shreeya kantamsetty and I am rising sophomore. I am interested in machine learning and data science. I have expertise in web development such as angular, css, and html html html html html. \"\n",
    "\n",
    "def filter_pitch (pitch): \n",
    "    pitch_tokens = word_tokenize(pitch)\n",
    "\n",
    "    punctuation=re.compile(r'[-,.?!,:;()|0-9]')\n",
    "\n",
    "    all_stopwords = stopwords.words('english')\n",
    "\n",
    "    filtered_pitch_tokens = [word for word in pitch_tokens if word not in all_stopwords]\n",
    "\n",
    "    no_punctuation = []\n",
    "    for words in filtered_pitch_tokens:\n",
    "        word=punctuation.sub(\"\",words)\n",
    "        if len(word)>0:\n",
    "            no_punctuation.append(word)\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    filtered_pitch = [lemmatizer.lemmatize(word.lower()) for word in no_punctuation]\n",
    "\n",
    "    return filtered_pitch\n",
    "\n",
    "\n",
    "filter_pitch(pitch)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are your overused words:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['html']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_frequency(pitch):\n",
    "    fdist = FreqDist()\n",
    "    for word in pitch:\n",
    "        fdist[word]+=1\n",
    "    overusedWords = []\n",
    "    for word, frequency in fdist.items():\n",
    "        if frequency > 3:\n",
    "            overusedWords.append(word)\n",
    "    if len(overusedWords) > 0:\n",
    "        print(\"Here are your overused words:\")\n",
    "        return overusedWords\n",
    "    else:\n",
    "        print(\"You have no overused words. Great job!\")\n",
    "\n",
    "\n",
    "find_frequency(filter_pitch(pitch))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexical richness\n",
    "\n",
    "##### Calculates lexical richness in terms of **total number of distinct words out of total number of words**\n",
    "\n",
    "* Type-Token Ratio(TTR): Our ratio calculates the # of unique words in proportion to the total number of words. Generally, a TTR between 0.2 and 0.4 is considered average, while a TTR above 0.4 is often seen as more diverse and rich. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is your lexical richness score: 0.1323529411764706\n",
      "You have low lexical richness. Try diversifying the ewords you are using.\n"
     ]
    }
   ],
   "source": [
    "def lexical_richness(pitch):\n",
    "    richness = len(set(pitch))/len(pitch)\n",
    "    print(\"Here is your lexical richness score:\", richness)\n",
    "    if (richness > 0.4):\n",
    "        print(\"Your text has has lexical richness!\")\n",
    "    else: \n",
    "        print(\"You have low lexical richness. Try diversifying the ewords you are using.\")\n",
    "\n",
    "lexical_richness(pitch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Skills from Elevator Pitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', 'name', 'shreeya', 'kantamsetty', 'i', 'sophomore', 'i', 'interested', 'machine', 'data', 'science', 'i', 'expertise', 'web', 'development', 'angular', 'cs', 'html', 'html', 'html', 'html', 'html']\n"
     ]
    }
   ],
   "source": [
    "tagged_tokens = pos_tag(filter_pitch(pitch))\n",
    "relevant_tags = ['NN', 'NNS', 'NNP', 'NNPS', 'JJ']\n",
    "skills = [token for token, tag in tagged_tokens if tag in relevant_tags]\n",
    "print(skills)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roberta Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0752e22d4524d62ac0696c26d78e7e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL=f\"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Here is your roberta_negative score': 0.004285957,\n",
       " 'Here is your roberta_neutral score': 0.3596528,\n",
       " 'Here is your roberta_positive score': 0.6360612}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def polarity_scores_roberta(example):\n",
    "    encoded_text = tokenizer(pitch, return_tensors = 'pt')\n",
    "    output = model(**encoded_text)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    scores_dict = {\n",
    "        'Here is your roberta_negative score' : scores[0],\n",
    "        'Here is your roberta_neutral score' : scores[1],\n",
    "        'Here is your roberta_positive score' : scores[2]\n",
    "    }\n",
    "    if (scores[0] > 0.2):\n",
    "        print(\"Your text is too negative. Try adding some positive words: \")\n",
    "    return scores_dict\n",
    "\n",
    "polarity_scores_roberta(pitch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
