{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Shreeya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/Shreeya/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.probability import FreqDist\n",
    "import re\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "from nltk import pos_tag\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering Input\n",
    "1. **Tokenization**: The word_tokenize function is applied to the pitch text, splitting it into individual characters.\n",
    "\n",
    "2. **Filtering and Processing**: The code removes punctuation and stopwords from the tokenized pitch using regular expressions (punctuation.sub) and a list of stopwords from the NLTK library (stopwords.words('english')).\n",
    "\n",
    "3. **Lemmatization**: The code utilizes the WordNetLemmatizer from NLTK (WordNetLemmatizer()) to lemmatize the list of words with no punctuation. Lemmatization reduces words to their root form. The resulting lemmas are stored in the filtered_pitch list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi',\n",
       " 'name',\n",
       " 'shreeya',\n",
       " 'kantamsetty',\n",
       " 'i',\n",
       " 'rising',\n",
       " 'sophomore',\n",
       " 'i',\n",
       " 'interested',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'data',\n",
       " 'science',\n",
       " 'i',\n",
       " 'expertise',\n",
       " 'web',\n",
       " 'development',\n",
       " 'angular',\n",
       " 'cs',\n",
       " 'html',\n",
       " 'html',\n",
       " 'html',\n",
       " 'html',\n",
       " 'html']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pitch = \"Hi, my name is shreeya kantamsetty and I am rising sophomore. I am interested in machine learning and data science. I have expertise in web development such as angular, css, and html html html html html. \"\n",
    "\n",
    "def filter_pitch (pitch): \n",
    "    pitch_tokens = word_tokenize(pitch)\n",
    "\n",
    "    punctuation=re.compile(r'[-,.?!,:;()|0-9]')\n",
    "\n",
    "    all_stopwords = stopwords.words('english')\n",
    "\n",
    "    filtered_pitch_tokens = [word for word in pitch_tokens if word not in all_stopwords]\n",
    "\n",
    "    no_punctuation = []\n",
    "    for words in filtered_pitch_tokens:\n",
    "        word=punctuation.sub(\"\",words)\n",
    "        if len(word)>0:\n",
    "            no_punctuation.append(word)\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    filtered_pitch = [lemmatizer.lemmatize(word.lower()) for word in no_punctuation]\n",
    "\n",
    "    return filtered_pitch\n",
    "\n",
    "\n",
    "filter_pitch(pitch)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are your overused words:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['html']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_frequency(pitch):\n",
    "    fdist = FreqDist()\n",
    "    for word in pitch:\n",
    "        fdist[word]+=1\n",
    "    overusedWords = []\n",
    "    for word, frequency in fdist.items():\n",
    "        if frequency > 3:\n",
    "            overusedWords.append(word)\n",
    "    if len(overusedWords) > 0:\n",
    "        print(\"Here are your overused words:\")\n",
    "        return overusedWords\n",
    "    else:\n",
    "        print(\"You have no overused words. Great job!\")\n",
    "\n",
    "\n",
    "find_frequency(filter_pitch(pitch))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexical richness\n",
    "\n",
    "##### Calculates lexical richness in terms of **total number of distinct words out of total number of words**\n",
    "\n",
    "* Type-Token Ratio(TTR): Our ratio calculates the # of unique words in proportion to the total number of words. Generally, a TTR between 0.2 and 0.4 is considered average, while a TTR above 0.4 is often seen as more diverse and rich. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is your lexical richness score: 0.1323529411764706\n",
      "You have low lexical richness. Try diversifying the ewords you are using.\n"
     ]
    }
   ],
   "source": [
    "def lexical_richness(pitch):\n",
    "    richness = len(set(pitch))/len(pitch)\n",
    "    print(\"Here is your lexical richness score:\", richness)\n",
    "    if (richness > 0.4):\n",
    "        print(\"Your text has has lexical richness!\")\n",
    "    else: \n",
    "        print(\"You have low lexical richness. Try diversifying the ewords you are using.\")\n",
    "\n",
    "lexical_richness(pitch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Skills from Elevator Pitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', 'name', 'shreeya', 'kantamsetty', 'i', 'sophomore', 'i', 'interested', 'machine', 'data', 'science', 'i', 'expertise', 'web', 'development', 'angular', 'cs', 'html', 'html', 'html', 'html', 'html']\n"
     ]
    }
   ],
   "source": [
    "tagged_tokens = pos_tag(filter_pitch(pitch))\n",
    "relevant_tags = ['NN', 'NNS', 'NNP', 'NNPS', 'JJ']\n",
    "skills = [token for token, tag in tagged_tokens if tag in relevant_tags]\n",
    "print(skills)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roberta Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nAutoModelForSequenceClassification requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/Shreeya/Desktop/codingProjects/pitchAnalyzer/analyze.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Shreeya/Desktop/codingProjects/pitchAnalyzer/analyze.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m MODEL\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcardiffnlp/twitter-roberta-base-sentiment\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Shreeya/Desktop/codingProjects/pitchAnalyzer/analyze.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(MODEL)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/Shreeya/Desktop/codingProjects/pitchAnalyzer/analyze.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelForSequenceClassification\u001b[39m.\u001b[39;49mfrom_pretrained(MODEL)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/transformers/utils/import_utils.py:1026\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[39mif\u001b[39;00m key\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m key \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_from_config\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1025\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1026\u001b[0m requires_backends(\u001b[39mcls\u001b[39;49m, \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_backends)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/transformers/utils/import_utils.py:1014\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1012\u001b[0m failed \u001b[39m=\u001b[39m [msg\u001b[39m.\u001b[39mformat(name) \u001b[39mfor\u001b[39;00m available, msg \u001b[39min\u001b[39;00m checks \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m available()]\n\u001b[1;32m   1013\u001b[0m \u001b[39mif\u001b[39;00m failed:\n\u001b[0;32m-> 1014\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nAutoModelForSequenceClassification requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "MODEL=f\"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/Shreeya/Desktop/codingProjects/pitchAnalyzer/analyze.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Shreeya/Desktop/codingProjects/pitchAnalyzer/analyze.ipynb#X20sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mYour text is too negative. Try adding some positive words: \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Shreeya/Desktop/codingProjects/pitchAnalyzer/analyze.ipynb#X20sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m scores_dict\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/Shreeya/Desktop/codingProjects/pitchAnalyzer/analyze.ipynb#X20sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m polarity_scores_roberta(pitch)\n",
      "\u001b[1;32m/Users/Shreeya/Desktop/codingProjects/pitchAnalyzer/analyze.ipynb Cell 13\u001b[0m in \u001b[0;36mpolarity_scores_roberta\u001b[0;34m(example)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Shreeya/Desktop/codingProjects/pitchAnalyzer/analyze.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpolarity_scores_roberta\u001b[39m(example):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/Shreeya/Desktop/codingProjects/pitchAnalyzer/analyze.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     encoded_text \u001b[39m=\u001b[39m tokenizer(pitch, return_tensors \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Shreeya/Desktop/codingProjects/pitchAnalyzer/analyze.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     output \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mencoded_text)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Shreeya/Desktop/codingProjects/pitchAnalyzer/analyze.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     scores \u001b[39m=\u001b[39m output[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "def polarity_scores_roberta(example):\n",
    "    encoded_text = tokenizer(pitch, return_tensors = 'pt')\n",
    "    output = model(**encoded_text)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    scores_dict = {\n",
    "        'Here is your roberta_neg score' : scores[0],\n",
    "        'Here is your roberta_neu score' : scores[1],\n",
    "        'Here is your roberta_pos score' : scores[2]\n",
    "    }\n",
    "    if (scores[0] > 0.2):\n",
    "        print(\"Your text is too negative. Try adding some positive words: \")\n",
    "    return scores_dict\n",
    "\n",
    "polarity_scores_roberta(pitch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
